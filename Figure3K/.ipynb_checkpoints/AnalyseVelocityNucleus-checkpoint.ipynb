{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory ='/DataVelocity_20210208'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond1 = 'ctl_apoptoticNuclei'\n",
    "cond2 = 'drug_apoptoticNuclei'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cond 1 = apoptoticnuclei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cond1 \n",
    "working_directory = os.path.join(directory, cond1)\n",
    "# List file in the working directory\n",
    "list_file = os.listdir(working_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine initial information into 3 tables according to each time interval acquisition\n",
    "df_60 = pd.DataFrame(np.nan*np.zeros((20,1)))\n",
    "df_80 = pd.DataFrame(np.nan*np.zeros((20,1)))\n",
    "df_90 = pd.DataFrame(np.nan*np.zeros((20,1)))\n",
    "\n",
    "for f in list_file :\n",
    "    df = pd.read_csv(os.path.join(working_directory, f), sep=';', encoding = \"latin\")\n",
    "    df.columns = df.columns.str.lower()\n",
    "    if df.loc[0]['time(s)']==60:\n",
    "        # remove line with NaN value = control information \n",
    "        df.dropna(inplace = True)\n",
    "        df_60[f.split('_')[0]] = df['norm veloc']\n",
    "        \n",
    "    if df.loc[0]['time(s)']==80:\n",
    "        # remove line with NaN value = control information \n",
    "        df.dropna(inplace = True)\n",
    "        df_80[f.split('_')[0]] = df['norm veloc']\n",
    "        \n",
    "    if df.loc[0]['time(s)']==90:\n",
    "        # remove line with NaN value = control information \n",
    "        df.dropna(inplace = True)\n",
    "        df_90[f.split('_')[0]] = df['norm veloc']    \n",
    "\n",
    "        \n",
    "# Remove the first column, which has be used to avoid trunkat data\n",
    "df_60.drop(0, axis=1, inplace=True)\n",
    "# Remove line which contains no information\n",
    "df_60.dropna(axis=0, how='all', inplace=True)\n",
    "# Transpose matrix\n",
    "df_60 = df_60.T\n",
    "# Rename column according to time interval\n",
    "nb_t = df_60.shape[1]\n",
    "columns = np.arange(0, 60*nb_t, 60)\n",
    "df_60.columns = columns\n",
    "\n",
    "# Remove the first column, which has be used to avoid trunkat data\n",
    "df_80.drop(0, axis=1, inplace=True)\n",
    "# Remove line which contains no information\n",
    "df_80.dropna(axis=0, how='all', inplace=True)\n",
    "# Transpose matrix\n",
    "df_80 = df_80.T\n",
    "# Rename column according to time interval\n",
    "nb_t = df_80.shape[1]\n",
    "columns = np.arange(0, 80*nb_t, 80)\n",
    "df_80.columns = columns\n",
    "\n",
    "# Remove the first column, which has be used to avoid trunkat data\n",
    "df_90.drop(0, axis=1, inplace=True)\n",
    "# Remove line which contains no information\n",
    "df_90.dropna(axis=0, how='all', inplace=True)\n",
    "# Transpose matrix\n",
    "df_90 = df_90.T\n",
    "# Rename column according to time interval\n",
    "nb_t = df_90.shape[1]\n",
    "columns = np.arange(0, 90*nb_t, 90)\n",
    "df_90.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there is information in the dataframe\n",
    "# Extrapolate velocity\n",
    "try:\n",
    "    df_60 = df_60.T\n",
    "    initial_time = df_60.index.to_list()\n",
    "    for i in range(0, df_60.index.max()+10,  10):\n",
    "        if i not in df_60.index : \n",
    "            df_60.loc[i] = df_60.loc[i//60*60]+(df_60.loc[(i//60+1)*60]-df_60.loc[i//60*60])/60*(i%60)\n",
    "\n",
    "    df_60.sort_index(inplace=True)\n",
    "    df_60.to_csv(directory+'/df_60_ctl_apoptotic.csv')\n",
    "\n",
    "# otherwise, do nothing\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there is information in the dataframe\n",
    "# Extrapolate velocity\n",
    "try:\n",
    "    df_80 = df_80.T\n",
    "    initial_time = df_80.index.to_list()\n",
    "    for i in range(0, df_80.index.max()+10,  10):\n",
    "        if i not in df_80.index : \n",
    "            df_80.loc[i] = df_80.loc[i//80*80]+(df_80.loc[(i//80+1)*80]-df_80.loc[i//80*80])/80*(i%80)\n",
    "    df_80.sort_index(inplace=True)\n",
    "    df_60.to_csv(directory+'/df_80_ctl_apoptotic.csv')\n",
    "\n",
    "# otherwise, do nothing\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there is information in the dataframe\n",
    "# Extrapolate velocity\n",
    "try :\n",
    "    df_90 = df_90.T\n",
    "    initial_time = df_90.index.to_list()\n",
    "    for i in range(0, df_90.index.max()+10,  10):\n",
    "        if i not in df_90.index : \n",
    "            df_90.loc[i] = df_90.loc[i//90*90]+(df_90.loc[(i//90+1)*90]-df_90.loc[i//90*90])/90*(i%90)\n",
    "            \n",
    "    df_90.sort_index(inplace=True)\n",
    "    df_90.to_csv(directory+'/df_90_ctl_apoptotic.csv')\n",
    "    \n",
    "# otherwise, do nothing\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the three tables into one\n",
    "df_all = df_60.copy()\n",
    "df_all = pd.concat((df_all, df_80), axis=1)\n",
    "df_all = pd.concat((df_all, df_90), axis=1)\n",
    "df_all.to_csv(directory+'/df_all_ctl_apoptotic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin-suz/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:61: FutureWarning: \n",
      "The current behaviour of 'Series.argmax' is deprecated, use 'idxmax'\n",
      "instead.\n",
      "The behavior of 'argmax' will be corrected to return the positional\n",
      "maximum in the future. For now, use 'series.values.argmax' or\n",
      "'np.argmax(np.array(values))' to get the position of the maximum\n",
      "row.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# Align on peak\n",
    "before_peak_align = pd.DataFrame(np.zeros((df_all.shape[0],1)))\n",
    "after_peak_align = pd.DataFrame(np.zeros((df_all.shape[0],1)))\n",
    "\n",
    "for col_name, values in df_all.iteritems():\n",
    "    before_peak_align[col_name] = pd.DataFrame(values[:np.argmax(values)][::-1].to_numpy())\n",
    "    after_peak_align[col_name] = pd.DataFrame(values[np.argmax(values):].to_numpy())\n",
    "    \n",
    "before_peak_align.drop(0, axis=1, inplace=True)\n",
    "before_peak_align.dropna(axis=0, how='all', inplace=True)\n",
    "after_peak_align.drop(0, axis=1, inplace=True)\n",
    "after_peak_align.dropna(axis=0, how='all', inplace=True)\n",
    "\n",
    "before_peak_align = before_peak_align[::-1]\n",
    "before_peak_align.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "align_result = pd.concat((before_peak_align, after_peak_align), axis=0)\n",
    "align_result.reset_index(inplace=True, drop=True)\n",
    "align_result.to_csv(directory+'/df_all_ctl_apoptotic_align.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cond 2 : 'non-apoptoticNuclei/xls'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cond2\n",
    "working_directory = os.path.join(directory, cond2)\n",
    "list_file = os.listdir(working_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dnRho1_20201013-Gc3ai_nlsscarlet_dnRHO2ug_FaiSc_e2_5ss_driftcorr-middle_nt1_combi.avi_tr.csv\n",
      "apo3_Y_20201002-Gc3ai_nlsscar_Y27632-100um_FaiSc_e2_part2_5ss_bot_nt1.csv\n",
      "apo10_Y_20201016-Gc3ai_nlsscarlet_20uMY27632_FaiSc_e2_part1_6ss_driftcorr-bottom_nt1_tr.csv\n",
      "apo11_Y_20201211-Gc3ai_nlsscarlet_FAiSc_Y27632_100uM_e2_driftcorr-bottom_nt1_tr.csv\n",
      "apo1_Y_20201002-Gc3ai_nlsscarlet_Y27632-100um_FaiSc_e1_part2_5ss_driftcorr-top_rot-16_nt1.csv\n",
      "dnRho3_20210126-GC3a-nlsScarlet_dnRho2ug_FAiSc_e2_part2_5ss_driftcorr-top_nt1_combi.avi_tr.csv\n",
      "dnRho2_20210126-GC3a-nlsScarlet_dnRho2ug_FAiSc_e1_part2_5ss_driftcorr-middle_nt2_combi.avi_tr.csv\n",
      "apo4_Y_20201009-Gc3ai_nlsscarlet_Y27632-100um_FaiSc_e1_6ss_nt1_tr.csv\n",
      "dnRho4_20210126-GC3a-nlsScarlet_dnRho2ug_FAiSc_e2_part2_5ss_driftcorr-top_nt2_combi.avi_tr.csv\n"
     ]
    }
   ],
   "source": [
    "# Combine initial information into 3 tables according to each time interval acquisition\n",
    "df_60 = pd.DataFrame(np.nan*np.zeros((20,1)))\n",
    "df_80 = pd.DataFrame(np.nan*np.zeros((20,1)))\n",
    "df_90 = pd.DataFrame(np.nan*np.zeros((20,1)))\n",
    "\n",
    "for f in list_file :\n",
    "    print(f)\n",
    "    df = pd.read_csv(os.path.join(working_directory, f), sep=';', encoding = \"latin\")\n",
    "    df.columns = df.columns.str.lower()\n",
    "    \n",
    "    if df.loc[0]['time (sec)']==60:\n",
    "        # remove line with NaN value = control information \n",
    "        df.dropna(inplace = True)\n",
    "        df_60[f.split('_')[0]] = df['norm veloc']\n",
    "    \n",
    "    if df.loc[0]['time (sec)']==80:\n",
    "        # remove line with NaN value = control information \n",
    "        df.dropna(inplace = True)\n",
    "        df_80[f.split('_')[0]] = df['norm veloc']\n",
    "        \n",
    "    if df.loc[0]['time (sec)']==90:\n",
    "        # remove line with NaN value = control information \n",
    "        df.dropna(inplace = True)\n",
    "        df_90[f.split('_')[0]] = df['norm veloc']\n",
    "        \n",
    "df_60.drop(0, axis=1, inplace=True)\n",
    "df_60.dropna(axis=0, how='all', inplace=True)\n",
    "df_60 = df_60.T\n",
    "nb_t = df_60.shape[1]\n",
    "columns = np.arange(0, 60*nb_t, 60)\n",
    "df_60.columns = columns\n",
    "\n",
    "df_80.drop(0, axis=1, inplace=True)\n",
    "df_80.dropna(axis=0, how='all', inplace=True)\n",
    "df_80 = df_80.T\n",
    "nb_t = df_80.shape[1]\n",
    "columns = np.arange(0, 80*nb_t, 80)\n",
    "df_80.columns = columns\n",
    "\n",
    "df_90.drop(0, axis=1, inplace=True)\n",
    "df_90.dropna(axis=0, how='all', inplace=True)\n",
    "df_90 = df_90.T\n",
    "nb_t = df_90.shape[1]\n",
    "columns = np.arange(0, 90*nb_t, 90)\n",
    "df_90.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "try :\n",
    "    df_60 = df_60.T\n",
    "    initial_time = df_60.index.to_list()\n",
    "    for i in range(0, df_60.index.max()+10,  10):\n",
    "        if i not in df_60.index : \n",
    "            df_60.loc[i] = df_60.loc[i//60*60]+(df_60.loc[(i//60+1)*60]-df_60.loc[i//60*60])/60*(i%60)\n",
    "\n",
    "    df_60.sort_index(inplace=True)\n",
    "    df_60.to_csv(directory+'/df_60_drug_apoptotic.csv')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_80 = df_80.T\n",
    "    initial_time = df_80.index.to_list()\n",
    "    for i in range(0, df_80.index.max()+10,  10):\n",
    "        if i not in df_80.index : \n",
    "            df_80.loc[i] = df_80.loc[i//80*80]+(df_80.loc[(i//80+1)*80]-df_80.loc[i//80*80])/80*(i%80)\n",
    "            \n",
    "    df_80.sort_index(inplace=True)\n",
    "    df_80.to_csv(directory+'/df_80_drug_apoptotic.csv')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_90 = df_90.T\n",
    "    initial_time = df_90.index.to_list()\n",
    "    for i in range(0, df_90.index.max()+10,  10):\n",
    "        if i not in df_90.index : \n",
    "            df_90.loc[i] = df_90.loc[i//90*90]+(df_90.loc[(i//90+1)*90]-df_90.loc[i//90*90])/90*(i%90)\n",
    "            \n",
    "    df_90.sort_index(inplace=True)\n",
    "    df_90.to_csv(directory+'/df_90_drug_apoptotic.csv')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_60.copy()\n",
    "df_all = pd.concat((df_all, df_80), axis=1)\n",
    "df_all = pd.concat((df_all, df_90), axis=1)\n",
    "df_all.to_csv(directory+'/df_all_drug_apoptotic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align on peak\n",
    "before_peak_align = pd.DataFrame(np.zeros((df_all.shape[0],1)))\n",
    "after_peak_align = pd.DataFrame(np.zeros((df_all.shape[0],1)))\n",
    "\n",
    "for col_name, values in df_all.iteritems():\n",
    "    before_peak_align[col_name] = pd.DataFrame(values[:np.argmax(values)][::-1].to_numpy())\n",
    "    after_peak_align[col_name] = pd.DataFrame(values[np.argmax(values):].to_numpy())\n",
    "    \n",
    "before_peak_align.drop(0, axis=1, inplace=True)\n",
    "before_peak_align.dropna(axis=0, how='all', inplace=True)\n",
    "after_peak_align.drop(0, axis=1, inplace=True)\n",
    "after_peak_align.dropna(axis=0, how='all', inplace=True)\n",
    "\n",
    "before_peak_align = before_peak_align[::-1]\n",
    "before_peak_align.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "align_result = pd.concat((before_peak_align, after_peak_align), axis=0)\n",
    "align_result.reset_index(inplace=True, drop=True)\n",
    "align_result.to_csv(directory+'/df_all_drug_apoptotic_align.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
